{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01412caf",
   "metadata": {},
   "source": [
    "# Hello Object Detection - Deployment\n",
    "\n",
    "A very basic introduction to using object detection models with OpenVINO™.\n",
    "\n",
    "The [horizontal-text-detection-0001](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/horizontal-text-detection-0001/README.md) model from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) is used. It detects horizontal text in images and returns a blob of data in the shape of `[100, 5]`. Each detected text box is stored in the `[x_min, y_min, x_max, y_max, conf]` format, where the\n",
    "`(x_min, y_min)` are the coordinates of the top left bounding box corner, `(x_max, y_max)` are the coordinates of the bottom right bounding box corner and `conf` is the confidence for the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141ecfe-0ac2-43f2-a6bd-e86ea48e234d",
   "metadata": {},
   "source": [
    "## 6. 배포\n",
    "\n",
    "- UI/UX 고려\n",
    "- 프로토 타입 형태 배포: OpenCV 프레임, Gradio, Streamlit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515dcb8a-b930-4246-aa57-dde47abe0035",
   "metadata": {},
   "source": [
    "### 6-1. Gradio\n",
    "https://www.gradio.app/guides/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73d7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e674a148-58a6-4717-b0ef-73f7caa83956",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPU', 'GPU']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "options=core.available_devices\n",
    "\n",
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99737c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = core.read_model(model=\"model/horizontal-text-detection-0001.xml\")\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(\"boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "334a83ff-cc41-48d9-8531-4f24edee622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "\n",
    "    image = np.array(image)  # Convert PIL image to numpy array\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "    \n",
    "    N, C, H, W = input_layer.shape\n",
    "    resized_image = cv2.resize(image, (W, H))\n",
    "    input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "    \n",
    "    return input_image, resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9122eb38-9af0-4877-a06a-73cc7c06079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_result_to_image(rgb_image, resized_image, boxes, threshold=0.3, conf_labels=True):\n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "        rgb_image.shape[:2],\n",
    "        resized_image.shape[:2],\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Iterate through non-zero boxes.\n",
    "    for box in boxes:\n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = box[-1]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image,\n",
    "            # position the upper box bar little lower to make it visible on the image.\n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                (int(max(corner_position * ratio_y, 10)) if idx % 2 else int(corner_position * ratio_x)) for idx, corner_position in enumerate(box[:-1])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb2f3a7f-1c23-4b8b-9811-13619b206a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    input_image, resized_image = preprocess(image)  # Preprocess the image\n",
    "    \n",
    "    # Create an inference request.\n",
    "    boxes = compiled_model([input_image])[output_layer]\n",
    "    # Remove zero only boxes.\n",
    "    boxes = boxes[~np.all(boxes == 0, axis=1)]\n",
    "    \n",
    "    canvas = convert_result_to_image(image, resized_image, boxes, conf_labels=False)\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "588c19a8-11d1-48ce-8bd8-5be739b10922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Gradio interface\n",
    "demo = gr.Interface(predict_image, gr.Image(), \"image\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80869f1-afb1-4cd3-9d08-93c389213d45",
   "metadata": {},
   "source": [
    "### 6-2. OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b4182f7-63a0-41e7-873b-273d9feb738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e339bb35-7a43-450f-a8be-103238ce7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = core.read_model(model=\"model/horizontal-text-detection-0001.xml\")\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(\"boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "642fcf2e-b401-42ae-ba9f-7cfa0c06afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "\n",
    "    image = np.array(image)  # Convert PIL image to numpy array\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert RGB to BGR for OpenCV\n",
    "    \n",
    "    N, C, H, W = input_layer.shape\n",
    "    resized_image = cv2.resize(image, (W, H))\n",
    "    input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "    \n",
    "    return input_image, resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b80f6f98-796f-48c0-bcda-692ac4f200a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_result_to_image(rgb_image, resized_image, boxes, threshold=0.3, conf_labels=True):\n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = (\n",
    "        rgb_image.shape[:2],\n",
    "        resized_image.shape[:2],\n",
    "    )\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Iterate through non-zero boxes.\n",
    "    for box in boxes:\n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = box[-1]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image,\n",
    "            # position the upper box bar little lower to make it visible on the image.\n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                (int(max(corner_position * ratio_y, 10)) if idx % 2 else int(corner_position * ratio_x)) for idx, corner_position in enumerate(box[:-1])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3fb82e53-e5b3-4993-a1b7-e80392f13d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, background_image_path):\n",
    "    input_image, resized_image = preprocess(image)  # Preprocess the image\n",
    "    \n",
    "    # Create an inference request.\n",
    "    boxes = compiled_model([input_image])[output_layer]\n",
    "    # Remove zero only boxes.\n",
    "    boxes = boxes[~np.all(boxes == 0, axis=1)]\n",
    "    \n",
    "    canvas = convert_result_to_image(image, resized_image, boxes, conf_labels=False)\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "897c2e59-6e84-4a10-a45e-b64b5b24d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, background_image_path):\n",
    "    # Read the input image\n",
    "    image = cv2.imread(image_path)\n",
    "    input_image, resized_image = preprocess(image)  # Preprocess the image\n",
    "    \n",
    "    # Create an inference request.\n",
    "    boxes = compiled_model([input_image])[output_layer]\n",
    "    # Remove zero only boxes.\n",
    "    boxes = boxes[~np.all(boxes == 0, axis=1)]\n",
    "    \n",
    "    canvas = convert_result_to_image(image, resized_image, boxes, conf_labels=False)\n",
    "\n",
    "# Read the background image\n",
    "    bg = cv2.imread(background_image_path)\n",
    "\n",
    "    # Resize the input image to match the proper position of background image\n",
    "    image_h, image_w = image.shape[0], image.shape[1]\n",
    "    new_h = 500\n",
    "    new_w = int((new_h/image_h)*image_w)\n",
    "    image_resize = cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "    \n",
    "    xmax = bg.shape[1] - 300\n",
    "    ymax = bg.shape[0] - 170\n",
    "    xmin = xmax - new_w\n",
    "    ymin = ymax - new_h\n",
    "    \n",
    "    # Overlay the input image on the background image\n",
    "    bg[ymin:ymax, xmin:xmax] = image_resize\n",
    "\n",
    "    # Display the final combined image\n",
    "    cv2.imshow(\"Sample Image with Prediction on Background\", bg)\n",
    "\n",
    "    # Wait for a key press and close the window\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "sample_image_path = \"./data/intel_rnb.jpg\"  # Replace with your actual image file path\n",
    "#sample_image_path = \"./data/starbucks.jpg\"  # Replace with your actual image file path\n",
    "background_image_path = \"./data/background.jpg\"  # Replace with your actual background image file path\n",
    "\n",
    "predict_image(sample_image_path, background_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10213b-5d1e-4213-b84b-23ebf35d135f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae617ccb002f72b3ab6d0069d721eac67ac2a969e83c083c4321cfcab0437cd1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/36741649/128489933-bf215a3f-06fa-4918-8833-cb0bf9fb1cc7.jpg",
   "tags": {
    "categories": [
     "First Steps"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Object Detection"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
